# -*- coding: utf-8 -*-
"""Seaborn final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vObdcmhnMb6jRqj0NIVijJjP_pSV0E8P

# It's a Fraud

Given details about a transaction determine whether the transaction is a Fraud or Not

## Dataset Description

In this competition you are predicting whether an online transaction is fraudulent or not

### Files

train.csv - the training set

test.csv - the test set

sample_submission.csv - a sample submission file in the correct format
Columns

`TransactionDT: timedelta from a given reference datetime (not an actual timestamp) “TransactionDT "corresponds to the number of seconds in a day.
TransactionAMT: transaction payment amount in USD

`ProductCD: product code, the product for each transaction

card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.

addr: address

“both addresses are for purchaser

addr1 as billing region

addr2 as billing country”

dist: distances between (not limited) billing address, mailing address, zip
code, IP address, phone area, etc.”

P_ and (R__) emaildomain: purchaser and recipient email domain

“ certain transactions don't need recipient, so R_emaildomain is null.”

C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc.

D1-D15: timedelta, such as days between previous transaction, etc.

M1-M9: match, such as names on card and address, etc.

Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.

“id01 to id11 are numerical features for identity.

Other columns such as network connection information (IP, ISP, Proxy, etc),digital signature (UA/browser/os/version, etc) associated with transactions are also present

Importing dependencies
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

#from google.colab import drive
#drive.mount('/content/drive')

"""Reading given training data into dataframe: train_df"""

train_df = pd.read_csv("../input/trainingdata/train.csv")
test_df = pd.read_csv("../input/testingdata/test.csv")

train_df.shape

"""This is moderately sized data. heavy preprocessing is required to extract meaningful features(columns) and remove faulty/ redundant data entries(rows).

--------------------------------------------------------------------------------------------------------------------------------------------------------------------
"""

train_df.head()

train_df.describe()

missing_values_count = train_df.isnull().sum()
print (missing_values_count[0:10])
total_cells = np.product(train_df.shape)
total_missing = missing_values_count.sum()
print ("% of missing data = ",(total_missing/total_cells) * 100)

catagorical_features=train_df.select_dtypes(include=['category','object']).columns
continuous_features = list(filter(lambda x: x not in catagorical_features, train_df))
catagorical_features

"""#EDA: Explanatory Data Analysis:

# isFraud - Target Distribution

Extreme imbalanced, only 3.53% of Transactions that are fraudulent in the train dataset.
"""

plt.subplots(figsize=(10,5))
sns.countplot(train_df['isFraud'])
plt.show()
print('From total data ',np.round(train_df[train_df['isFraud']==1].shape[0]/train_df.shape[0]*100,2),'% contains fraud train')
print('From total data ',np.round(train_df[train_df['isFraud']==0].shape[0]/train_df.shape[0]*100,2),'% contains legit train')

"""# ProductCD"""

plt.figure(figsize=(6,6))
train_ProductCD = (train_df.groupby(['isFraud'])['ProductCD'].value_counts(normalize=True).rename('percentage').mul(100).reset_index().sort_values('ProductCD'))
sns.barplot(x="ProductCD", y="percentage", hue="isFraud", data=train_ProductCD,palette = 'pastel')
plt.title('Percentage of fraud and legit across ProductCD in Train')
plt.show()

"""From the graph above we can see that if the transaction is fraudulent, there is 40% chance it's Product C (We can't say vice versa). Considering that only 10% of Legit transaction is C, I think there is higher chance that it's a fraud if it's product is C.

# Transaction amount
"""

plt.figure(figsize=(17,6))
sns.kdeplot(train_df[train_df['isFraud']==0]['TransactionAmt'])
sns.kdeplot(train_df[train_df['isFraud']==1]['TransactionAmt'])
plt.ylabel('Probability Density')
plt.legend(['legit','fraud'])
plt.title('Train')
plt.suptitle('TransactionAmt Distribution' , fontsize=12)

train_df['TransactionAmt'] = np.log(train_df['TransactionAmt'])
test_df['TransactionAmt'] = np.log(test_df['TransactionAmt'])

plt.figure(figsize=(12,6))
sns.kdeplot(train_df[train_df['isFraud']==0]['TransactionAmt'])
sns.kdeplot(train_df[train_df['isFraud']==1]['TransactionAmt'])
plt.xticks(np.arange(-2, 10, 1))
plt.ylabel('Probability Density')
plt.legend(['legit','fraud'])
plt.title('Train')
plt.suptitle('TransactionAmt Distribution with log' , fontsize=12)
plt.show()

"""The transactions with LogTransactionAmt larger than 5.5 (244 dollars) and smaller than 3.3 (27 dollars) have higher frequency and probability density being fraudulent. On the other hand, LogTransactionAmt from 3.3 to 5.5 have higher chance being legit.

# Analyzing DeviceType
"""

print(len(train_df['DeviceType'].value_counts()))

plt.figure(figsize=(12,6))
plt.subplot(1,2,1)
train_df_DeviceType = (train_df[~train_df['DeviceType'].isnull()].groupby(['isFraud'])['DeviceType'].value_counts(normalize=True).rename('percentage').mul(100).reset_index().sort_values('DeviceType'))
sns.barplot(x="DeviceType", y="percentage", hue="isFraud", data=train_df_DeviceType, palette = 'pastel')
plt.title('Percentage of fraud across DeviceType')
plt.show()

"""Both Desktop and mobile have the same percentage of fraud

# addr1 and addr2
"""

train_df['addr2'].hist()

train_df['addr1'].hist()

train_df = train_df.drop(['addr2'],axis=1)
test_df = test_df.drop(['addr2'],axis=1)

"""as 4 lakh rows have same value for addr2 col, we will drop it 
as variance is very low

# Analyzing C1 to C14

As we can see in the above plot they all are highly right skewed so there must be outliers lets remove some of them.
"""

f, axes = plt.subplots(7, 2, figsize=(15,30))

cols = [f"C{i}" for i in range(1, 15)]

for col, ax in zip(cols, axes.flat):
    sns.kdeplot(data=train_df, x=col, hue="isFraud", fill=True, ax=ax)

plt.tight_layout()

"""# Preprocessing"""

test_df.isna().sum() / len(test_df) * 100

train_df.isna().sum() / len(train_df) * 100

"""Removing rows with columns having less than 1 percent NULL values."""

train_df.drop(axis="rows", labels=train_df.index[train_df['card2'].isna()], inplace=True)
train_df.drop(axis="rows", labels=train_df.index[train_df['card3'].isna()], inplace=True)
train_df.drop(axis="rows", labels=train_df.index[train_df['card4'].isna()], inplace=True)
train_df.drop(axis="rows", labels=train_df.index[train_df['card5'].isna()], inplace=True)
train_df.drop(axis="rows", labels=train_df.index[train_df['card6'].isna()], inplace=True)
train_df.drop(axis="rows", labels=train_df.index[train_df['D1'].isna()], inplace=True)
train_df.drop(axis="rows", labels=train_df.index[train_df['V95'].isna()], inplace=True)
train_df.drop(axis="rows", labels=train_df.index[train_df['V279'].isna()], inplace=True)
train_df.drop(axis="rows", labels=train_df.index[train_df['V281'].isna()], inplace=True)

"""Removing Columns with more than 70 percent NULL values."""

train_df = train_df.drop(columns =train_df[ list(train_df.loc[:,'V138':'V278']) + ['dist2','R_emaildomain','D12','D13','D14','DeviceType','DeviceInfo'] + list(train_df.loc[:,'D6':'D9']) + list(train_df.loc[:,'V322':'V339']) + list(train_df.loc[:,'id_01':'id_38'])])

test_df = test_df.drop(columns =test_df[ list(test_df.loc[:,'V138':'V278']) + ['dist2','R_emaildomain','D12','D13','D14','DeviceType','DeviceInfo'] + list(test_df.loc[:,'D6':'D9']) + list(test_df.loc[:,'V322':'V339']) + list(test_df.loc[:,'id_01':'id_38'])])

"""Now filling the remaining column NULL values accordingly."""

train_df.isna().sum()

train_df['dist1'].describe()

train_df['dist1'] = train_df['dist1'].fillna(train_df['dist1'].median())
test_df['dist1'] = test_df['dist1'].fillna(test_df['dist1'].median())
train_df['dist1'].hist()

train_df['addr1'].fillna(train_df['addr1'].mode().iloc[0], inplace=True)
test_df['addr1'].fillna(test_df['addr1'].mode().iloc[0], inplace=True)

train_df['P_emaildomain'] = train_df['P_emaildomain'].fillna('NA')
test_df['P_emaildomain'] = test_df['P_emaildomain'].fillna('NA')

train_df.loc[:,'D1':'D15'] = train_df.loc[:,'D1':'D15'].fillna(train_df.loc[:,'D1':'D15'].mean())
test_df.loc[:,'D1':'D15'] = test_df.loc[:,'D1':'D15'].fillna(test_df.loc[:,'D1':'D15'].mean())

train_df.loc[:,'V1':'V94'] = train_df.loc[:,'V1':'V94'].fillna(train_df.loc[:,'V1':'V94'].mean())
test_df.loc[:,'V1':'V321'] = test_df.loc[:,'V1':'V321'].fillna(test_df.loc[:,'V1':'V321'].mean())

train_df.isna().sum()

train_df.loc[:,'M1':'M9'] = train_df.loc[:,'M1':'M9'].replace({'T': 0 ,'F': 1})
train_df['M4'] = train_df['M4'].replace({'M0': 0 ,'M1': 1 , 'M2': 2})

test_df.loc[:,'M1':'M9'] = test_df.loc[:,'M1':'M9'].replace({'T': 0 ,'F': 1})
test_df['M4'] = test_df['M4'].replace({'M0': 0 ,'M1': 1 , 'M2': 2})

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()

train_df['ProductCD']=label_encoder.fit_transform(train_df['ProductCD'])
train_df['card4']=label_encoder.fit_transform(train_df['card4'])
train_df['card6']=label_encoder.fit_transform(train_df['card6'])
train_df['P_emaildomain']=label_encoder.fit_transform(train_df['P_emaildomain'])

test_df['ProductCD']=label_encoder.fit_transform(test_df['ProductCD'])
test_df['card4']=label_encoder.fit_transform(test_df['card4'])
test_df['card6']=label_encoder.fit_transform(test_df['card6'])
test_df['P_emaildomain']=label_encoder.fit_transform(test_df['P_emaildomain'])

train_df = train_df.drop(columns = ['TransactionID'])

test_df = test_df.drop(columns = ['TransactionID'])

train_df.duplicated().sum()

train_df.drop(axis="rows", labels=train_df.index[train_df.duplicated()], inplace=True)
train_df.duplicated().sum()

train_df.head()

train_df['M1'].value_counts(normalize=True)

train_df['M1'] = train_df['M1'].fillna(pd.Series(np.random.choice([0,1], p=[0.99, 0.01], size=len(train_df))))
test_df['M1'] = test_df['M1'].fillna(pd.Series(np.random.choice([0,1], p=[0.99, 0.01], size=len(test_df))))

train_df['M2'].value_counts(normalize=True)

train_df['M2'] = train_df['M2'].fillna(pd.Series(np.random.choice([0,1], p=[0.89, 0.11], size=len(train_df))))
test_df['M2'] = test_df['M2'].fillna(pd.Series(np.random.choice([0,1], p=[0.89, 0.11], size=len(test_df))))

train_df['M3'].value_counts(normalize=True)

train_df['M3'] = train_df['M3'].fillna(pd.Series(np.random.choice([0,1], p=[0.78, 0.22], size=len(train_df))))
test_df['M3'] = test_df['M3'].fillna(pd.Series(np.random.choice([0,1], p=[0.78, 0.22], size=len(test_df))))

train_df['M4'].value_counts(normalize=True)

train_df['M4'] = train_df['M4'].fillna(pd.Series(np.random.choice([0,2,1], p=[0.63, 0.20, 0.17], size=len(train_df))))
test_df['M4'] = test_df['M4'].fillna(pd.Series(np.random.choice([0,2,1], p=[0.63, 0.20, 0.17], size=len(test_df))))

train_df['M5'].value_counts(normalize=True)

train_df['M5'] = train_df['M5'].fillna(pd.Series(np.random.choice([0,1], p=[0.44, 0.56], size=len(train_df))))
test_df['M5'] = test_df['M5'].fillna(pd.Series(np.random.choice([0,1], p=[0.44, 0.56], size=len(test_df))))

train_df['M6'].value_counts(normalize=True)

train_df['M6'] = train_df['M6'].fillna(pd.Series(np.random.choice([0,1], p=[0.45, 0.55], size=len(train_df))))
test_df['M6'] = test_df['M6'].fillna(pd.Series(np.random.choice([0,1], p=[0.45, 0.55], size=len(test_df))))

train_df['M7'].value_counts(normalize=True)

train_df['M7'] = train_df['M7'].fillna(pd.Series(np.random.choice([0,1], p=[0.13, 0.87], size=len(train_df))))
test_df['M7'] = test_df['M7'].fillna(pd.Series(np.random.choice([0,1], p=[0.13, 0.87], size=len(test_df))))

train_df['M8'].value_counts(normalize=True)

train_df['M8'] = train_df['M8'].fillna(pd.Series(np.random.choice([0,1], p=[0.36, 0.64], size=len(train_df))))
test_df['M8'] = test_df['M8'].fillna(pd.Series(np.random.choice([0,1], p=[0.36, 0.64], size=len(test_df))))

train_df['M9'].value_counts(normalize=True)

train_df['M9'] = train_df['M9'].fillna(pd.Series(np.random.choice([0,1], p=[0.84, 0.16], size=len(train_df))))
test_df['M9'] = test_df['M9'].fillna(pd.Series(np.random.choice([0,1], p=[0.84, 0.16], size=len(test_df))))

train_df.loc[:,'M1':'M9'] = train_df.loc[:,'M1':'M9'].fillna(train_df.mode().iloc[0])
test_df.loc[:,'M1':'M9'] = test_df.loc[:,'M1':'M9'].fillna(test_df.mode().iloc[0])
train_df.isna().sum()

test_df.loc[:,'card2':'card5'] = test_df.loc[:,'card2':'card5'].fillna(test_df.loc[:,'card2':'card5'].mean())

test_df.isna().sum()

train_df.shape

"""-------------------------------------------------------------------------------------------------------------------------------------------------------------

## Removing columns with variance less than 1 percent
"""

threshold = 0.1

train_df=train_df.drop(train_df.std()[train_df.std() < threshold].index.values, axis = 1)

test_df=test_df.drop(test_df.std()[test_df.std() < threshold].index.values, axis = 1)

train_df.shape

test_df.shape

train_df.describe()

"""Removing columns with correlation > 0.90

as they are somewhat linearly dependent so they will not bring any new information to the data.
"""

# Create correlation matrix
corr_matrix = train_df.corr().abs()

# Select upper triangle of correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Find features with correlation greater than 0.95
to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]

# Drop features
train_df.drop(to_drop, axis=1, inplace=True)

test_df.drop(to_drop, axis=1, inplace=True)

train_df.describe()

train_df.shape

cat_cols = ["ProductCD", "card1", "card2", "card3", "card4", "card5", "card6", "addr1","addr2", "P_emaildomain","M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9"]

num_cols = [col for col in train_df.columns if col not in cat_cols and col not in ["isFraud","TransactionAmt"]]

len(num_cols)

X= train_df.drop(['isFraud'],axis=1)
Y=train_df['isFraud']

train_col = X.columns
test_col = test_df.columns

"""Normalization of data"""

# # data normalization with sklearn

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X = scaler.fit_transform(X)

test_df = scaler.transform(test_df)

X = pd.DataFrame(X, columns=train_col)
test_df = pd.DataFrame(test_df, columns=test_col)

X.shape

fraud = train_df[train_df['isFraud']==1]
normal = train_df[train_df['isFraud']==0]

print(fraud.shape,normal.shape)

from imblearn.under_sampling import RandomUnderSampler

under = RandomUnderSampler()
X_res,y_res=under.fit_resample(X,Y)

X_res.shape,y_res.shape

test_df.shape

from collections import Counter
print('Original dataset shape {}'.format(Counter(Y)))
print('Resampled dataset shape {}'.format(Counter(y_res)))

"""## Let's try to apply Logistic Regression"""

from sklearn.linear_model import LogisticRegression

param_grid = [    
    {'penalty' : ['l1', 'l2'],
    'C' : [.1],
    'solver' : ['saga','liblinear'],
     'max_iter' : [5000, 10000]
     
    }
]

from sklearn.model_selection import GridSearchCV
logreg = LogisticRegression()
clf = GridSearchCV(logreg, param_grid = param_grid, cv= 3,verbose=True, n_jobs=-1)

clf.fit(X_res, y_res)

print('\n All results:')
print(clf.cv_results_)
print('\n Best estimator:')
print(clf.best_estimator_)
#print('\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))
print(clf.best_score_ * 2 - 1)
print('\n Best hyperparameters:')
print(clf.best_params_)
results = pd.DataFrame(clf.cv_results_)
results.to_csv('lr-grid-search-results-01.csv', index=False)

y_pred = clf.predict(test_df)

y_prediction = pd.DataFrame(y_pred,columns=['isFraud'])

y_prediction.value_counts()[1]

id = []

y_pred.shape

for i in range(147635):
    id.append(i)

Id = pd.DataFrame(id, columns = ['Id'])

frames = [Id, y_prediction]
final_df = pd.concat(frames, axis=1)

final_df.head()

final_df.to_csv('final.csv',index=False)

"""# Applying Xgboost"""

from xgboost import XGBClassifier

#XGBoost hyper-parameter tuning
def hyperParameterTuning(X_train, y_train):
    param_tuning = {
        'learning_rate': [0.01, 0.1],
        'max_depth': [20, 10],
        'min_child_weight': [1, 3],
        'colsample_bytree': [0.5 , 0.4],
    }

    xgb_model = XGBClassifier()

    gsearch = GridSearchCV(estimator = xgb_model,
                           param_grid = param_tuning,                        
                           cv = 5,
                           n_jobs = -1,
                           verbose = 1)

    gsearch.fit(X_train,y_train)

    return gsearch.best_params_

#Run only in the first run of the kernel.
hyperParameterTuning(X_res, y_res)

# fit model no training data
model = XGBClassifier( colsample_bytree = 0.4,learning_rate = 0.1,max_depth = 20,min_child_weight = 1,gamma = 0.3)
model.fit(X_res, y_res)

y_pred_xg = model.predict(test_df)

y_xg = pd.DataFrame(y_pred_xg,columns=['isFraud'])

y_xg.value_counts()[1]

y_xg.shape

frames = [Id, y_xg]
xg_df = pd.concat(frames, axis=1)

xg_df.head()

xg_df.to_csv('xg_final.csv',index=False)

"""# Random Forest"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

rfc=RandomForestClassifier(random_state=42)

param_grid = { 
    'n_estimators': [50,200],
    'max_features': ['sqrt', 'log2'],
    'min_samples_split' : [2,3],
    'max_samples' : [1,2]
}

CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 2)
CV_rfc.fit(X_res, y_res)

print('\n All results:')
print(CV_rfc.cv_results_)
print('\n Best estimator:')
print(CV_rfc.best_estimator_)
#print('\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))
print(CV_rfc.best_score_ * 2 - 1)
print('\n Best hyperparameters:')
print(CV_rfc.best_params_)
results = pd.DataFrame(CV_rfc.cv_results_)
results.to_csv('lr-grid-search-results-01.csv', index=False)

from sklearn.ensemble import RandomForestClassifier
# Random Forest Classifier
rfc = RandomForestClassifier(criterion='entropy', max_features='sqrt', max_samples=0.5, min_samples_split=2)
rfc.fit(X_res, y_res)

predictions = rfc.predict(test_df)

y_rf = pd.DataFrame(predictions,columns=['isFraud'])

y_rf.value_counts()[1]

y_rf.shape

frames = [Id, y_rf]
rf_df = pd.concat(frames, axis=1)

rf_df.head()

rf_df.to_csv('rf_final.csv',index=False)

"""# Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()
y_pred = gnb.fit(X_res, y_res).predict(test_df)

y_nb = pd.DataFrame(y_pred,columns=['isFraud'])

y_nb.value_counts()[1]

frames = [Id, y_nb]
nb_df = pd.concat(frames, axis=1)

nb_df.head()

nb_df.to_csv('nb_final.csv',index=False)

"""# Decision tree"""

# Create the parameter grid based on the results of random search 
params = {
    'max_depth': [20,50],
    'min_samples_leaf': [20, 50],
    'criterion': ["gini", "entropy"]
}

# Instantiate the grid search model
from sklearn import tree
dt = tree.DecisionTreeClassifier(random_state = 42)
clf = GridSearchCV(estimator=dt, 
                    param_grid=params, 
                    cv=4, n_jobs=-1, verbose=1)

clf = clf.fit(X_res, y_res)
y_pred = clf.predict(test_df)

print('\n All results:')
print(clf.cv_results_)
print('\n Best estimator:')
print(clf.best_estimator_)
#print('\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))
print(clf.best_score_ * 2 - 1)
print('\n Best hyperparameters:')
print(clf.best_params_)
results = pd.DataFrame(clf.cv_results_)
results.to_csv('lr-grid-search-results-01.csv', index=False)

y_dt = pd.DataFrame(y_pred,columns=['isFraud'])

y_dt.value_counts()[1]

frames = [Id, y_dt]
dt_df = pd.concat(frames, axis=1)

dt_df.head()

dt_df.to_csv('dt_final.csv',index=False)

"""# KNN"""

from sklearn.neighbors import KNeighborsClassifier

grid_params = {'n_neighbors' : [5,6],
               'weights' : ['uniform','distance']}
neigh = GridSearchCV(KNeighborsClassifier(), grid_params, verbose = 1, cv=3, n_jobs = -1)

neigh.fit(X_res, y_res)
y_pred = neigh.predict(test_df)

print('\n All results:')
print(neigh.cv_results_)
print('\n Best estimator:')
print(neigh.best_estimator_)
#print('\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))
print(neigh.best_score_ * 2 - 1)
print('\n Best hyperparameters:')
print(neigh.best_params_)
results = pd.DataFrame(neigh.cv_results_)
results.to_csv('lr-grid-search-results-01.csv', index=False)

y_knn = pd.DataFrame(y_pred,columns=['isFraud'])

y_knn.value_counts()[1]

frames = [Id, y_knn]
knn_df = pd.concat(frames, axis=1)

knn_df.head()

knn_df.to_csv('knn_final.csv',index=False)

"""# Applying SVM"""

from sklearn import svm

# defining parameter range
param_grid = {'C': [ 1,3], 
              'gamma': [0.1, 0.01],
              'kernel': ['rbf']} 
  
clf = GridSearchCV(svm.SVC(), param_grid, refit = True, verbose = 3)

clf.fit(X_res, y_res)
y_pred = clf.predict(test_df)

print('\n All results:')
print(clf.cv_results_)
print('\n Best estimator:')
print(clf.best_estimator_)
#print('\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))
print(clf.best_score_ * 2 - 1)
print('\n Best hyperparameters:')
print(clf.best_params_)
results = pd.DataFrame(clf.cv_results_)
results.to_csv('lr-grid-search-results-01.csv', index=False)

y_svm = pd.DataFrame(y_pred,columns=['isFraud'])

y_svm.value_counts()[1]

frames = [Id, y_svm]
svm_df = pd.concat(frames, axis=1)

svm_df.head()

svm_df.to_csv('svm_final.csv',index=False)

"""# NN"""

from sklearn.neural_network import MLPClassifier

clf = MLPClassifier()
clf.fit(X_res, y_res)
y_pred=clf.predict(test_df)

y_nn = pd.DataFrame(y_pred,columns=['isFraud'])

y_nn.value_counts()[1]

frames = [Id, y_nn]
nn_df = pd.concat(frames, axis=1)

nn_df.head()

nn_df.to_csv('nn_final.csv',index=False)